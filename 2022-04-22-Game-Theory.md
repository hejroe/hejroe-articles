# Game Theory
 
Monopoly is a game that many will be familiar with. It was created to demonstrate the inherent weaknesses of a capitalistic system. In a normal game there is a rule that allows for two players to make any deal providing they both consent. Now what if we were to construct games where the participants had been selected due to character traits? Psychology offers some help here with the “Big 5” personality traits.

For our first game we will have four players who are all very disagreeable. This means that they will act in their own self interests. So how does the game play out? I like to imagine that the “best” outcome from the perspective of one of the participants is to of course win. But perhaps the reality is even more extreme with one “boss”, the “guard” and two “slaves”. In this instance there is a clear “winner”.

So, what about a game of Monopoly played by four players who are all very agreeable? With this type of personality, they are likely to put the interests of others over that of their own. In this instance is it possible that all the parties make an agreement to just circle the board and not enforce any of the financial penalties? Agreed it is a far cry from the original intent of the game, but it would also seem to be a perfectly plausible outcome of the game. One could likely argue that in this game all the players won as they did not have to endure the tedium of a normal game of monopoly.

With these two game sessions we see very different outcomes despite the environment being the same in both cases. The only difference in how the two games unfolded was in how the participants approached the problem of “solving” the game.

To take this example even further we could say that there are about 100,000 different combinations of the “Big 5” personality traits for a single player. With big numbers there are often many possible outcomes. It is even possible that in all the possible permutations that one of the games will conclude with everyone being happy. Ridiculous I know, but at least theoretically possible.

Game theory came about, at least as far as I am concerned, as the attempt of a formal description of games with the specific intent of using that knowledge for personal gain. Since then, it has morphed and encroached on other fields of study. With perhaps unsurprisingly economics being a keen proponent of such ideas.

The issue with game theory almost always comes from rational agents with clearly defined goals. Humans for instance are not rational agents. Not even when you think you have everything lined up perfectly will a human do what is expected. That perhaps is kind of the point of being human.

But there is a great deal of value that has come from the efforts of study in this area. There are very definitely strategies for optimising commonly agreed goals within specific constraints. At the very least it can be a mechanism to overcome decision paralysis that seems to hinder many organisations.

## Making the AI game safe

With the creation of AI systems, you have to consider safety of the system. That is what happens when things go wrong? I have no doubt that they will go wrong as we have seen that happen already. A good example of this comes from the “Tit for Tat” strategy in game theory. In this we see an incredibly effective system that is very simple in its form. Start from a “co-operative” state and then just do whatever is done to you. So simple. But when this very simple system breaks it will not become co-operative again until it breaks again, which would “fix” the first break and put it back into the co-operative state.

This is far from an ideal situation. To have a system break and remain functioning in an errant way is not often desirable. This is even more the case when the errant behaviour is hard to spot or only becomes apparent after a period of time. In fact, it is not completely unreasonable to state that this type of situation must be avoided. We cannot have rogue automatons going about the place, accomplishing tasks that should not be accomplished.

To address this, it is necessary to create systems that fail into a safe state. Going back to the “tit for tat” example. In the “better” version we see a mechanism introduced that accounts for forgiveness. In doing so we have created a mechanism for self-correction should an error of a certain type occur. But in adding this new feature, we also add complexity.

For instance, we now must track the relationships that we have with every entity we interact with. With this comes new variables that need to be maintained. This then creates more chances for errors to creep into the system. What if our “forgiving” function stops forgiving? How does that get corrected? and what will happen in the meantime?

Perhaps a better way to negate such issues is to create systems that avoid unwanted behaviour while being in the simplest form possible. So “forgiving tit for tat” does try to do this but introduces other opportunities for errors to occur. But is there a simpler mechanism available?

Let us step through a scenario to see what components are present.

Forgiving functionality requires that we know:

the identity of each object we are interacting with.
A counter that is set to a level of “forgiveness”.
A mechanism to decrement/increment the counter.
Overheads become obvious if we consider numerous interactions with any number of objects. For each object interacted with an identity and counter must be set and then tracked. So how can we make a simpler system which is still functionally the same, but fails into a safe state? Could the solution be as simple as being forgetful?

Now this may seem fanciful, as surely a forgetful component introduces a temporal element that was not required before. But what if the “forgetfulness” was as a result of something more physical in nature? In this I am hinting at Neuromorphic computing. This field of computing is founded on making computing materials similar to those found in nature. Think in terms of information being in close proximity to the processes that use it. Memory layered with processing capacity in the same media.

Using neuromorphic computing we would only create a new local connection when encountering a new object that we can interact with. This connection would be refreshed or strengthened each time an interaction was made with the same object. Over time the connection will degrade if no further interactions are made. In essence the forgetfulness component.

By having such a solution, you now have a different component with which to design systems. Ones that would help us from creating the basis of our own undoing.

## The green fields of learning.

Already we have seen AI systems redefine how games are played. The systems show us new ways of thinking about games and the strategies to be used. The games Go, and chess have both seen significant impact from AI systems. It would in fact be relatively safe to say that we have now lost our claim to be the domain experts in both.

This does bring us to a place where re-thinking old problems and applying the latest technology could easily reveal some overlooked founding principal. Truly exciting times.
